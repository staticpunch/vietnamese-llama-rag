{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff421c5-5863-4c21-b70f-10a9daed0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import sent_tokenize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=100, window_size=50):\n",
    "    \"\"\"Split a long text into multiple chunks (passages) with managable sizes.\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): Maximum size of a chunk.\n",
    "        window_size (int): Decide how many words are overlapped between two consecutive chunks. Basically #overlapped_words = chunk_size - window_size.\n",
    "    Returns:\n",
    "        str: Multiple chunks of text splitted from initial document text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "\n",
    "    while True:\n",
    "        end_idx = start_idx + chunk_size\n",
    "        chunk = \" \".join(words[start_idx:end_idx])\n",
    "        chunks.append(chunk)\n",
    "        if end_idx >= num_words:\n",
    "            break\n",
    "        start_idx += window_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_corpus(data_dir=\"data/data_raw10k/\"):\n",
    "    \"\"\"Transform a corpus of documents into a corpus of passages.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory that contains .txt files, each file contains text content of a wikipedia page.\n",
    "    Returns:\n",
    "        str: A corpus of chunks splitted from multiple initial documents. Each chunk will contain information about (id, title, passage)\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    meta_corpus = []\n",
    "    data_dir = \"data/data_raw10k/\"\n",
    "    filenames = os.listdir(data_dir)\n",
    "    filenames = sorted(filenames)\n",
    "    \n",
    "    _id = 0\n",
    "    docs = {}\n",
    "    for filename in tqdm(filenames):\n",
    "        filepath = data_dir + filename\n",
    "        title = filename.strip(\".txt\")\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "            docs[title] = text\n",
    "            text = text.lstrip(title).strip()\n",
    "\n",
    "            # No overlap.\n",
    "            chunks = split_text_into_chunks(text, chunk_size=150, window_size=150)\n",
    "            chunks = [f\"Title: {title}\\n\\n{chunk}\" for chunk in chunks]\n",
    "            meta_chunks = [{\n",
    "                \"title\": title,\n",
    "                \"passage\": chunks[i],\n",
    "                \"id\": _id + i,\n",
    "                \"len\": len(chunks[i].split())\n",
    "            } for i in range(len(chunks))]\n",
    "            _id += len(chunks)\n",
    "            corpus.extend(chunks)\n",
    "            meta_corpus.extend(meta_chunks)\n",
    "    return meta_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf4da0f-df77-4f3e-adb1-7dbae684745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5932.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Corpus size: 48532\n",
      ">>> Example passage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'ATR 72',\n",
       " 'passage': 'Title: ATR 72\\n\\nngầm (ASW) của ATR 72-500 (chính nó là một phiên bản của biến thể tuần tra biển của ATR 42-500) cũng đang được chế tạo và đã được Hải quân Thổ Nhĩ Kỳ lựa chọn làm cho nhiệm vụ ASW và chống tàu ngầm (ASuW). Tổng cộng 10 chiếc sẽ được chuyển giao cho Hải quân Thổ Nhĩ Kỳ tới năm 2010. Chiếc máy bay sẽ được trang bị các tên lửa không đối đất và thủy lôi cho các nhiệm vụ SuW và ASW. Chúng cũng được trang bị các hệ thống chiến tranh điện tử và trinh sát và cũng sẽ được dùng cho nhiệm vụ tìm kiếm và cứu hộ trên biển. Hãng điều hành chính. Khoảng 47 hãng hàng không khác cũng đang sử dụng ở số lượng nhỏ hơn. Các công ty đặt hàng chính gồm: Thông tin Thêm. Điều này giúp giảm nhẹ trọng',\n",
       " 'id': 267,\n",
       " 'len': 153}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_corpus = get_corpus()\n",
    "print(f\">>> Corpus size: {len(meta_corpus)}\")\n",
    "print(f\">>> Example passage\")\n",
    "meta_corpus[267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159b0f8b-51c3-4f66-997e-d3652b800f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/corpus_chunks.jsonl\", \"w\") as outfile:\n",
    "    for chunk in meta_corpus:\n",
    "        d = json.dumps(chunk, ensure_ascii=False) + \"\\n\"\n",
    "        outfile.write(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f7264a-069a-4cd3-82e5-92e6af06093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48532/48532 [01:33<00:00, 517.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder').cuda()\n",
    "segmented_corpus = [tokenize(example[\"passage\"]) for example in tqdm(meta_corpus)]\n",
    "embeddings = model.encode(segmented_corpus)\n",
    "embeddings /= np.linalg.norm(embeddings, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8ebaa2-9e0e-40ea-b80f-2d00b6650213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/corpus_embedding_w150.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
