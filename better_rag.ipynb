{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec878b7-0e6e-4727-bcd0-699769498e85",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421f9af-4aff-43f1-9f78-a1aa9ec1e65e",
   "metadata": {},
   "source": [
    "First we have to load the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8495631a-a621-4b9f-9338-0d3a2af6e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "meta_corpus = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"data/corpus_chunks.jsonl\",\n",
    "    split=\"train\"\n",
    ").to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626ec522-142c-4c29-8361-6d084bb6e513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3f2de786cb41a09e8c5e5ba8aa2100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import unicodedata as ud\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "def split_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if len(word.strip()) > 0]\n",
    "    return words\n",
    "\n",
    "## initiate BM25 retriever\n",
    "tokenized_corpus = [split_text(doc[\"passage\"]) for doc in tqdm(meta_corpus)]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06fbefd-21ec-4df4-833e-88b40e9b5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "## initiate semantic rertiever\n",
    "with open('data/corpus_embedding_w150.pkl', 'rb') as f:\n",
    "    corpus_embs = pickle.load(f)\n",
    "\n",
    "embedder = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e6063-bb2b-4d08-be11-824900cb19a5",
   "metadata": {},
   "source": [
    "To improve from simple single BM25 retriever, we now incorporate a sentence transformer model to do the semantic search. \\\n",
    "We will use `bkai-foundation-models/vietnamese-bi-encoder` model which supports Vietnamese pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9401c537-2a4f-4e82-a49b-49e93ce92979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def retrieve(question, topk=50):\n",
    "    \"\"\"\n",
    "    Get most relevant chunks to the question using combination of BM25 and semantic scores.\n",
    "    \"\"\"\n",
    "    ## initialize query for each retriever (BM25 and semantic)\n",
    "    tokenized_query = split_text(question)\n",
    "    segmented_question = tokenize(question)\n",
    "    question_emb = embedder.encode([segmented_question])\n",
    "    question_emb /= np.linalg.norm(question_emb, axis=1)[:, np.newaxis]\n",
    "\n",
    "    ## get BM25 and semantic scores\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    semantic_scores = question_emb @ corpus_embs.T\n",
    "    semantic_scores = semantic_scores[0]\n",
    "\n",
    "    ## update chunks' scores. \n",
    "    max_bm25_score = max(bm25_scores)\n",
    "    min_bm25_score = min(bm25_scores)\n",
    "    def normalize(x):\n",
    "        return (x - min_bm25_score + 0.1) / \\\n",
    "        (max_bm25_score - min_bm25_score + 0.1)\n",
    "        \n",
    "    corpus_size = len(meta_corpus)\n",
    "    for i in range(corpus_size):\n",
    "        meta_corpus[i][\"bm25_score\"] = bm25_scores[i]\n",
    "        meta_corpus[i][\"bm25_normed_score\"] = normalize(bm25_scores[i])\n",
    "        meta_corpus[i][\"semantic_score\"] = semantic_scores[i]\n",
    "\n",
    "    ## compute combined score (BM25 + semantic)\n",
    "    for passage in meta_corpus:\n",
    "        passage[\"combined_score\"] = passage[\"bm25_normed_score\"] * 0.4 + \\\n",
    "                                    passage[\"semantic_score\"] * 0.6\n",
    "\n",
    "    ## sort passages by the combined score\n",
    "    sorted_passages = sorted(meta_corpus, key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    return sorted_passages[:topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b31a43-eba2-4a97-9a61-db6e22dd442f",
   "metadata": {},
   "source": [
    "## Smoothing contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3e2d6-574b-42f0-9029-dbcd464b3c3b",
   "metadata": {},
   "source": [
    "Using combination of BM25 and semantic score may still not yield the best result (because of the reasons you will see at the end of this section) \\\n",
    "We can do better by applying several techniques to form suitable contexts to feed to the LLM. \\\n",
    "The following blocs will implement some utility functions with such techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cc319e-5a4e-4701-996f-3758b8b1efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from underthesea import sent_tokenize\n",
    "def extract_consecutive_subarray(numbers):\n",
    "    subarrays = []\n",
    "    current_subarray = []\n",
    "    for num in numbers:\n",
    "        if not current_subarray or num == current_subarray[-1] + 1:\n",
    "            current_subarray.append(num)\n",
    "        else:\n",
    "            subarrays.append(current_subarray)\n",
    "            current_subarray = [num]\n",
    "\n",
    "    subarrays.append(current_subarray)  # Append the last subarray\n",
    "    return subarrays\n",
    "    \n",
    "def merge_contexts(passages):\n",
    "    passages_sorted_by_id = sorted(passages, key=lambda x: x[\"id\"], reverse=False)\n",
    "    # psg_texts = [x[\"passage\"].strip(\"Title: \").strip(x[\"title\"]).strip() \n",
    "    #              for x in passages_sorted_by_id]\n",
    "    \n",
    "    psg_ids = [x[\"id\"] for x in passages_sorted_by_id]\n",
    "    consecutive_ids = extract_consecutive_subarray(psg_ids)\n",
    "\n",
    "    merged_contexts = []\n",
    "    consecutive_psgs = []\n",
    "    b = 0\n",
    "    for ids in consecutive_ids:\n",
    "        psgs = passages_sorted_by_id[b:b+len(ids)]\n",
    "        psg_texts = [x[\"passage\"].strip(\"Title: \").strip(x[\"title\"]).strip() \n",
    "                     for x in psgs]\n",
    "        merged = f\"Title: {psgs[0]['title']}\\n\\n\" + \" \".join(psg_texts)\n",
    "        b = b+len(ids)\n",
    "        merged_contexts.append(dict(\n",
    "            title=psgs[0]['title'], \n",
    "            passage=merged,\n",
    "            score=max([x[\"combined_score\"] for x in psgs]),\n",
    "            merged_from_ids=ids\n",
    "        ))\n",
    "    return merged_contexts\n",
    "\n",
    "def discard_contexts(passages):\n",
    "    sorted_passages = sorted(passages, key=lambda x: x[\"score\"], reverse=False)\n",
    "    if len(sorted_passages) == 1:\n",
    "        return sorted_passages\n",
    "    else:\n",
    "        shortened = deepcopy(sorted_passages)\n",
    "        for i in range(len(sorted_passages) - 1):\n",
    "            current, next = sorted_passages[i], sorted_passages[i+1]\n",
    "            if next[\"score\"] - current[\"score\"] >= 0.05:\n",
    "                shortened = sorted_passages[i+1:]\n",
    "        return shortened\n",
    "\n",
    "def expand_context(passage, word_window=60, n_sent=3):\n",
    "    # psg_id = passage[\"id\"]\n",
    "    merged_from_ids = passage[\"merged_from_ids\"]\n",
    "    title = passage[\"title\"]\n",
    "    prev_id = merged_from_ids[0] - 1\n",
    "    next_id = merged_from_ids[-1] + 1\n",
    "    strip_title = lambda x: x[\"passage\"].strip(f\"Title: {x['title']}\\n\\n\")\n",
    "    \n",
    "    texts = []\n",
    "    if prev_id in range(0, len(meta_corpus)):\n",
    "        prev_psg = meta_corpus[prev_id]\n",
    "        if prev_psg[\"title\"] == title: \n",
    "            prev_text = strip_title(prev_psg)\n",
    "            # prev_text = \" \".join(prev_text.split()[-word_window:])\n",
    "            prev_text = \" \".join(sent_tokenize(prev_text)[-n_sent:])\n",
    "            texts.append(prev_text)\n",
    "            \n",
    "    texts.append(strip_title(passage))\n",
    "    \n",
    "    if next_id in range(0, len(meta_corpus)):\n",
    "        next_psg = meta_corpus[next_id]\n",
    "        if next_psg[\"title\"] == title: \n",
    "            next_text = strip_title(next_psg)\n",
    "            # next_text = \" \".join(next_text.split()[:word_window])\n",
    "            next_text = \" \".join(sent_tokenize(next_text)[:n_sent])\n",
    "            texts.append(next_text)\n",
    "\n",
    "    expanded_text = \" \".join(texts)\n",
    "    expanded_text = f\"Title: {title}\\n{expanded_text}\"\n",
    "    new_passage = deepcopy(passage)\n",
    "    new_passage[\"passage\"] = expanded_text\n",
    "    return new_passage\n",
    "\n",
    "def expand_contexts(passages, word_window=60, n_sent=3):\n",
    "    new_passages = [expand_context(passage) for passage in passages]\n",
    "    return new_passages\n",
    "    \n",
    "def collapse(passages):\n",
    "    new_passages = deepcopy(passages)\n",
    "    titles = {}\n",
    "    for passage in new_passages:\n",
    "        title = passage[\"title\"]\n",
    "        if not titles.get(title):\n",
    "            titles[title] = [passage]\n",
    "        else:\n",
    "            titles[title].append(passage)\n",
    "    best_passages = []\n",
    "    for k, v in titles.items():\n",
    "        best_passage = max(v, key= lambda x: x[\"score\"])\n",
    "        best_passages.append(best_passage)\n",
    "    return best_passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9b0c7-5f8d-460f-aab7-33faa2dd8c13",
   "metadata": {},
   "source": [
    "Note that, with our current chunking strategy, each chunk is a passage of exact 150 words (separated by space), not a comprehensive paragraph. The following function will transform retrieved chunks into whole paragraphs. This function also does some heuristics to expand the context window and discard seem-to-be irrelevant contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f914149b-52de-46b5-b212-e30e31116ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_contexts(passages):\n",
    "    \"\"\"Make the context fed to the LLM better.\n",
    "    Args:\n",
    "        passages (list): Chunks retrieved from BM25 + semantic retrieval. \n",
    "        \n",
    "    Returns:\n",
    "        list: List of whole paragraphs, usually will be more relevant to the initital question.\n",
    "    \"\"\"\n",
    "    # 1. If consecutive chunks are rertieved, merge them into one big chunk to ensure the continuity.\n",
    "    merged_contexts = merge_contexts(passages)\n",
    "    # 2. A heuristic to discard irrelevevant contexts. \n",
    "    # It seems to be better to only keep what are elevant so that the model can focus.\n",
    "    # Also this reduce #tokens LLM has to read.\n",
    "    shortlisted_contexts = discard_contexts(merged_contexts)\n",
    "    # 3. Another heuristic. this step is to take advantage of long context understanding of the LLM.\n",
    "    # In many cases, the retrieved passages are just consecutive words, not a comprehensive paragraph.\n",
    "    # This is to expand the passage to the whole paragraph that surrounds it. \n",
    "    # My intuition about this is that whole paragraph will add necessary and relevant information.\n",
    "    expanded_contexts = expand_contexts(shortlisted_contexts)\n",
    "    # 4. Now after all the merging and expanding, if what are left for us is more than one paragraphs\n",
    "    # from the same wiki page, then we will only take paragraph with highest retrieval score.\n",
    "    collapsed_contexts = collapse(expanded_contexts)\n",
    "    return collapsed_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9fb2f5e-0992-4419-b6d8-c9ac03ed8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I encourage you to investigate each smoothing step using the following example\n",
    "## to understand the benefit of them. \n",
    "## You will see that after each step, we will obtain \"better\" contexts.\n",
    "\n",
    "question = \"Hồ Chí Minh là ai?\"\n",
    "top_passages = retrieve(question, topk=3)\n",
    "merged_contexts = merge_contexts(top_passages)\n",
    "shortlisted_contexts = discard_contexts(merged_contexts)\n",
    "expanded_contexts = expand_contexts(shortlisted_contexts)\n",
    "collapsed_contexts = collapse(expanded_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11f2fbf-7c1f-4854-89dd-e937f20afb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Gia đình Hồ Chí Minh',\n",
       "  'passage': 'Title: Gia đình Hồ Chí Minh\\n, tên thật là Nguyễn Sinh Cung, vị chủ tịch đầu tiên của nước Việt Nam Dân chủ Cộng hòa (nay là Cộng hòa xã hội chủ nghĩa Việt Nam), sinh ra trong một gia đình nhà Nho ở làng Sen (hay làng Kim Liên), xã Kim Liên, huyện Nam Đàn, tỉnh Nghệ An. Thân sinh. Nguyễn Sinh Sắc. Nguyễn Sinh Sắc (còn gọi là Nguyễn Sinh Huy, người dân còn gọi là Cụ Phó bảng; 1862 – 1929) là cha của Hồ Chí Minh. Ông là con của ông Nguyễn Sinh Nhậm và bà Hà Thị Hy. Lớn lên trong một môi trường Nho học dưới sự nuôi dạy của nhà Nho và cha vợ của mình là cụ Hoàng Xuân Đường, ông đỗ cử nhân năm 1894 và Phó bảng năm 1901. Năm 1906, ông được triều đình bổ nhiệm chức Thừa biện bộ Lễ; năm 1909, ông nhậm chức Tri huyện Bình Khê tỉnh Bình Định. Làm quan được ít lâu thì bị triều đình thải hồi vì một \"tên cường hào\" bị ông bắt giam rồi được thả. Sau đó ông đi vào miền Nam và sinh sống tại Làng Hòa An, Cao Lãnh, Đồng Tháp (nay là xã Hòa An, thành phố Cao Lãnh) cho đến cuối đời. Hoàng Thị Loan. Hoàng Thị Loan (1868—1901) là thân mẫu của Hồ Chí Minh. Bà là con gái của cụ Hoàng Xuân Đường, được ông gả vào năm 15 tuổi. Sau khi chồng bà là ông Nguyễn Sinh Sắc đi thi ở Huế, vì túng thiếu tiền bạc nên ngỏ ý mời bà lên kinh giúp ông học tập, bà đã gửi con gái đầu lòng của mình lại Nghệ An, rồi đưa hai con trai là Nguyễn Sinh Khiêm (7 tuổi) và Nguyễn S ung (5 tuổi) cùng chồng vào Huế. Ở đây, bà làm nghề dệt vải để trang trải cuộc sống vật chất cho gia đình. Năm 1900, sau khi sinh người con thứ tư là Nguyễn Sinh Nhuận cộng với sự vất vả khó nhọc trước đó, Hoàng Thị Loan sinh bệnh rồi qua đời vào ngày 10 tháng 2 năm 1901, là ngày 22 tháng Chạp năm Canh Tý.',\n",
       "  'score': 0.7354560781596656,\n",
       "  'merged_from_ids': [12868, 12869]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Uncomment each of these variable to see the differences.\n",
    "\n",
    "# top_passages\n",
    "# merged_contexts\n",
    "# shortlisted_contexts\n",
    "# expanded_contexts\n",
    "collapsed_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c5481-32a5-469c-9c78-d56c97ce7c1a",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1607d3-c1b6-4806-867a-3b0aa2cb24ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    \"### System:\\n\"\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n\"\n",
    "    \"### Input:\\n{input}\\n\\n\"\n",
    "    \"### Response:\\n{output}\"\n",
    ")\n",
    "\n",
    "def get_prompt(question, contexts):\n",
    "    context = \"\\n\\n\".join([f\"Context [{i+1}]: {x['passage']}\" for i, x in enumerate(contexts)])\n",
    "    instruction = 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.'\n",
    "    # instruction = 'As an intelligent AI model, your task is to analyze and integrate information from multiple contexts given below in order to answer questions and provide citations.'\n",
    "    input = f\"Dựa vào một số ngữ cảnh được cho dưới đây, trả lời câu hỏi ở cuối.\\n\\n{context}\\n\\nQuestion: {question}\\nHãy trả lời chi tiết và đầy đủ.\"\n",
    "    prompt = prompt_template.format(\n",
    "        instruction=instruction,\n",
    "        input=input,\n",
    "        output=''\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2901ae4-673c-428b-9800-4a76e9f2c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.\n",
      "\n",
      "### Input:\n",
      "Dựa vào một số ngữ cảnh được cho dưới đây, trả lời câu hỏi ở cuối.\n",
      "\n",
      "Context [1]: Title: Gia đình Hồ Chí Minh\n",
      ", tên thật là Nguyễn Sinh Cung, vị chủ tịch đầu tiên của nước Việt Nam Dân chủ Cộng hòa (nay là Cộng hòa xã hội chủ nghĩa Việt Nam), sinh ra trong một gia đình nhà Nho ở làng Sen (hay làng Kim Liên), xã Kim Liên, huyện Nam Đàn, tỉnh Nghệ An. Thân sinh. Nguyễn Sinh Sắc. Nguyễn Sinh Sắc (còn gọi là Nguyễn Sinh Huy, người dân còn gọi là Cụ Phó bảng; 1862 – 1929) là cha của Hồ Chí Minh. Ông là con của ông Nguyễn Sinh Nhậm và bà Hà Thị Hy. Lớn lên trong một môi trường Nho học dưới sự nuôi dạy của nhà Nho và cha vợ của mình là cụ Hoàng Xuân Đường, ông đỗ cử nhân năm 1894 và Phó bảng năm 1901. Năm 1906, ông được triều đình bổ nhiệm chức Thừa biện bộ Lễ; năm 1909, ông nhậm chức Tri huyện Bình Khê tỉnh Bình Định. Làm quan được ít lâu thì bị triều đình thải hồi vì một \"tên cường hào\" bị ông bắt giam rồi được thả. Sau đó ông đi vào miền Nam và sinh sống tại Làng Hòa An, Cao Lãnh, Đồng Tháp (nay là xã Hòa An, thành phố Cao Lãnh) cho đến cuối đời. Hoàng Thị Loan. Hoàng Thị Loan (1868—1901) là thân mẫu của Hồ Chí Minh. Bà là con gái của cụ Hoàng Xuân Đường, được ông gả vào năm 15 tuổi. Sau khi chồng bà là ông Nguyễn Sinh Sắc đi thi ở Huế, vì túng thiếu tiền bạc nên ngỏ ý mời bà lên kinh giúp ông học tập, bà đã gửi con gái đầu lòng của mình lại Nghệ An, rồi đưa hai con trai là Nguyễn Sinh Khiêm (7 tuổi) và Nguyễn S ung (5 tuổi) cùng chồng vào Huế. Ở đây, bà làm nghề dệt vải để trang trải cuộc sống vật chất cho gia đình. Năm 1900, sau khi sinh người con thứ tư là Nguyễn Sinh Nhuận cộng với sự vất vả khó nhọc trước đó, Hoàng Thị Loan sinh bệnh rồi qua đời vào ngày 10 tháng 2 năm 1901, là ngày 22 tháng Chạp năm Canh Tý.\n",
      "\n",
      "Question: Hồ Chí Minh là ai?\n",
      "Hãy trả lời chi tiết và đầy đủ.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Let's see how a prompt fed to the LLM looks like\n",
    "question = \"Hồ Chí Minh là ai?\"\n",
    "top_passages = retrieve(question, topk=3)\n",
    "smoothed_contexts = smooth_contexts(top_passages)\n",
    "prompt = get_prompt(question, smoothed_contexts)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de25f03a-c389-4891-80be-e9d7e6ec5f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb1a71343984d398b7a989f2b260615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig\n",
    "import torch\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "model_id = \"llm4fun/vietrag-7b-v1.0\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=LlamaConfig.from_pretrained(model_id),\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b325ceaf-bd57-4e44-9273-00b4b02abdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=1024):\n",
    "    \"\"\"Text completion with a given prompt. In other words, give answer to your question.\n",
    "    Args:\n",
    "        prompt (str): Basically <instruction> + <question> + <retrieved_context>\n",
    "        generation_config (not existed yet): For now, please manually tweak hyperparameters\n",
    "        for generation in the `generation_config` below. Uncomment necessary arguments as you wish.\n",
    "    Returns:\n",
    "        list: an answer to the question within the prompt.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # temperature=0.2,\n",
    "            # top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e709cb1-99f9-41be-8ff8-c247317b8ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hồ Chí Minh là một nhà lãnh đạo và chính trị gia người Việt Nam, sinh ra với tên khai sinh là Nguyễn Sinh Cung vào ngày 19 tháng 5 năm 1890 tại làng Sen (hay làng Kim Liên), xã Kim Liên, huyện Nam Đàn, tỉnh Nghệ An. Ông là con trai của Nguyễn Sinh Sắc (còn gọi là Nguyễn Sinh Huy hoặc Cụ Phó bảng) và Hoàng Thị Loan. Cha ông là một nhà Nho và là một quan chức triều đình, trong khi mẹ ông là một người phụ nữ có trách nhiệm và có trách nhiệm. Hồ Chí Minh đã được nuôi dưỡng trong một môi trường Nho học và đã đỗ cử nhân vào năm 1894 và Phó bảng vào năm 1901.</s>\n"
     ]
    }
   ],
   "source": [
    "## Let's test what the LLM would generate given a question and its context via a prompt.\n",
    "output = generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973e52e-9bf4-472e-b7be-de23e56a40e6",
   "metadata": {},
   "source": [
    "Not the best but pretty accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3b594-ab67-4ab9-a613-40be7f4cd7e6",
   "metadata": {},
   "source": [
    "## End-to-End RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210283cb-c199-4ced-b2b4-d14d79c0b7f2",
   "metadata": {},
   "source": [
    "It's almost done. Now let's try a simple RAG pipeline with our Wikipedia corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bc1f9c-80d1-4747-8e48-2c8cd18b4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question, topk=3):\n",
    "    top_passages = retrieve(question, topk=topk)\n",
    "    smoothed_contexts = smooth_contexts(top_passages)\n",
    "    retrieved_context = \"\\n\\n\".join([f\"Context [{i+1}]: {x['passage']}\" \n",
    "                    for i, x in enumerate(smoothed_contexts)])\n",
    "    prompt = get_prompt(question, smoothed_contexts)\n",
    "    output = generate(prompt)\n",
    "    result = {\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"generated_answer\": output\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6bceca-2c12-45b5-8cbc-f08f87556626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu hỏi: Thành phố nào là thủ đô của Việt Nam?\n",
      "Thành phố thủ đô của Việt Nam là Hà Nội. Nó được xếp hạng là đô thị đặc biệt và là trung tâm tổng hợp cấp quốc gia, cấp vùng hoặc cấp tỉnh về kinh tế, tài chính, văn hóa, giáo dục, đào tạo, du lịch, y tế, khoa học và công nghệ, đầu mối giao thông, giao lưu trong nước và quốc tế [1].</s>\n",
      "------------------------------------------------------------------------------------------\n",
      "Câu hỏi: Samsung do ai sở hữu?\n",
      "Samsung Electronics Co., Ltd. hiện được sở hữu hoàn toàn bởi Tập đoàn Samsung Group, một conglomerate kinh doanh toàn diện có trụ sở tại Hàn Quốc. Tập đoàn Samsung Group được thành lập bởi Lee Byung-Chull, người đã thành lập Công ty Samsung vào năm 1938. Sau khi ông qua đời vào năm 1987, con trai ông, Lee Kun-Hee, đã tiếp quản vai trò lãnh đạo của công ty và đã dẫn dắt công ty trở thành một trong những công ty lớn nhất thế giới. Ngày nay, Samsung Electronics Co., Ltd. là một trong những công ty lớn nhất thế giới, với các sản phẩm và dịch vụ như điện thoại thông minh, máy tính xách tay, TV, camera và các sản phẩm điện tử khác.</s>\n",
      "------------------------------------------------------------------------------------------\n",
      "Câu hỏi: Việt Nam có những ca sĩ KPOP nào?\n",
      "Có, Việt Nam có những ca sĩ KPOP. Một số ca sĩ KPOP Việt Nam bao gồm Lisa, một ca sĩ, MC và nhà thiết kế thời trang người Hàn Quốc, người đã được công chúng Việt Nam biết đến sau khi tham dự Celine Fashion Show cho bộ sưu tập Xuân Hè 2020 tại Paris, Pháp, trong Tuần lễ Thời trang Paris. Cô cũng được xếp vào danh sách 20 \"Nghệ sĩ Hàn Quốc được yêu thích nhất ở nước ngoài\" của Bộ Văn hóa, Thể thao và Du lịch và Quỹ Giao lưu Văn hóa Quốc ế Hàn Quốc trong cuộc khảo sát \"Cuộc khảo sát làn sóng Hàn Quốc ở nước ngoài năm 2021\", thu thập dữ liệu từ 18 quốc gia trên toàn thế giới. Lisa cũng được bổ nhiệm làm đại sứ thương hiệu toàn cầu của MAC Cosmetics và đã tạo ra 506 nghìn USD trong MIV® với bài đăng thông báo của cô trên Instagram với tư cách là đại sứ toàn cầu của MAC Cosmetics.</s>\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Thành phố nào là thủ đô của Việt Nam?\",\n",
    "    \"Samsung do ai sở hữu?\",\n",
    "    \"Việt Nam có những ca sĩ KPOP nào?\",\n",
    "]\n",
    "for question in questions:\n",
    "    print(f\"Câu hỏi: {question}\")\n",
    "    output = rag(question)\n",
    "    print(\"---\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
